🙟 The Syllabic Keyboard Project 🙝


𐫱 Goal 𐫱

Develop an iOS application that lets Inuktitut speakers frictionlessly type in
syllabics.

Additional goals:
– Let users write Inuktitut in latin text, using the same system as syllabics.
– Let users convert syllabic ⇋ latin text quickly in the suggestions bar.
– Let users pick suggestions from the aforementioned bar.

Non goals:
– 🗙 Facilitate writing in English, French, or any other latin-based language.
– 🗙 Facilitate writing in Cree, Ojibwe, or any other syllabic-based language.
– 🗙 Facilitate use of emojis, other characters not found on standard keyboards.


𐫱 Motive 𐫱

Syllabics are widely used by Inuktitut speakers.

Nunavik            → ᓄᓇᕕᒃ
Natsilingmiutut    → ᓇᑦᓯᖕᒥᐅᑐᑦ
Inuktitut          → ᐃᓄᒃᑎᑐᑦ
Nunatsiavummiutut  → ᓄᓇᑦᓯᐊᕗᒻᒥᐅᑐᑦ

Latin scripts are also used for all Inuit languages, but tend to produce words
that are long and unwieldy. The effectiveness of syllabics can be debated, but
the fact remains that they are highly popular today.

Unfortunately, despite good Unicode support, there exist no high-tier syllabic
keyboards for mobile devices.

I believe that by taking inspiration from other keyboards, particularly the
Japanese keyboard on iOS, writing in Inuktitut can go from being a slow and
painful process to a fast and enjoyable one. Furthermore, the same keyboard
design could be easily adapted to write the latin script at a much faster rate
than using a standard English keyboard.

𐫱 𐫱 𐫱 𐫱 𐫱

I currently lack the hardware to develop an iOS app, and as such I have to
consider my current capabilities. Concepts for two seperate systems arise:

1. A Rust-based system that will serve as the core logic for my iOS application,
as well as a future Android application; and

2. An HTML/CSS based system, which will serve as my rapid protoype.


For the web application, I have apprehensions. Namely, the type of code, and the
way everything is fit together, is unlikely to resemble, in any form, any sort
of Swift UI code on iOS. That being said, I'm well acquainted with these
technologies, and I think it would be a great opportunity for rapid prototyping
and experimentation.

For the Rust-based system, things have been going very slowly. I believe this is
due to their not being too much logical code, and my fear that once I'm finished
the internals, I'll be unable to work on the external code any time soon, due to
lack of hardware. I would very much like to finish my Rust-based syllabic
converter, but I think it might be more worth it to focus on the web app for
now.

Perhaps I should try and create a greenfield project to isolate the behaviour of
individual keys. Namely, the pressing of the keys and the sliding motion. This
could be achieved fairly easily with JS.

a: The single key, unpressed.
b: The dialogue that appears around the key when held.

a)    b)
    → ABC
 E  → DEF
    → GHI

The behaviour should be fairly simple, and less prine to scope creep than the
syllabic converter, so it could be a good start. I already have an HTML/CSS
mockup of the keyboard, though it's not functional. Once this is done, I can
combine the two and see how things work.

Eventually, I could add the suggestions bar, etc.


𐫱 Constraints 𐫱

It's very difficult for me not to over-optimize.

For the parsing code, I must use a regex-based tokenization system for the
lexing, and the regex library must be the standard one.

The lexer will determine if a given word is likely Inuktut, using some very
basic heuristics that can be coded into regex tokenizer itself. Ie. email
adresses and Zip codes should be left as is, perhaps even all all-caps
abbreviations. There's a desire to make this functionality very clever, but a
user is going to be using the converter on small selections of text most of the
time, and will expect simple, predictable behaviour.

Only words comprised of only Latin Inuktuk characters can go on to the parsing
stage, and the parser should not fail to produce output at that point (as in,
the conversion function *can* fail, but given the filtered input, that should
not be expected).

The regex to determine if a word is Inuktut will not pass any additional
information to the conversion function, that function can do it's own parsing.


Today, I will write this lexer. (December 26th, 2023)



𐫱 Misc. ideas 𐫱

Backspace deletes one character, always. But backspace+swipe left means you
delete the previous word/suggestion that was placed.

Look at this project to allow for morpheme-level suggestions:
https://github.com/LowResourceLanguages/InuktitutComputing


===


TODO:
Decide the strategy: should all series be included in the base table, given
the series is not equivocal to another (ie, different versions of the
H-series), or should the base table truly be the base, and the special
characters be in their own groups?

Ie. The "łii → ᖡ" conversion only applies to certain dialects technically,
but the conversion should be included (in both directions) for any dialect,
because neither the latin consonant nor the syllabic symbol are ambiguous.
The characters may be used in names, for example, and there's no point
leaving them out.

With the "cascading hashtables" idea, the "ł" series should be applied on
top of the base table, casting its shadow onto it. But the base table could
easily have this character, and that would allow it to exist in all dialects.
Another idea is to always have the base table not be the true root, and have
additional dialect tables trailing it. This seems wasteful.

Indeed, the idea of having multiple, cascading hashtables may be wasteful.
I'd like to consider having one table per dialect, and perhaps even two, one
with the ai column using the restored syllabics like ᐁ, and one version with
the ᐊᐃ. Having the cascading effect is tempting in this case, but I'd be
forced to create a ᐊᐃ or ᐁ for every version of the table, which just adds
work, confusion, and CPU cycles.

Really, the differentiation in dialects comes down to, as I understand it, the
H series (plural).

ᕼᐊ → Nunavut
𑪴 → Natsilik
ᓴ → Eastern Nunavut (but S in Western Nunavut)
ᕹ → Nunavik

I'd like to look at webscrapes and see which of these characters are commonly
used.

In any case, the cascading hashtable, if necessary, is simple and elegant.
I have my doubts that it's needed, however.

Perhaps the "patching" can just be user preferences? They don't have to be
perfect hashing, just a regular hashtable. For example, latin-izing the ᖓ
series as "ng" vs "ŋ".

TODO: Figure out what "ᐂ" is and add it, probably. 

===

Have tables of symbols that are converted from, but not too. For example,
𑪶 is preferred to ᓯ̵, the latter should be converted from, but not to. Likewise,
ł is preferred to ɫ in latin text. 

Number of bytes for keys? Just count the number of [backslash]x per line.

Reverse shadowing: have the basic table contain all unambiguous Inuktitut
mappings, then just have a hole for the ambiguous mappings (ie. H-series),
and place the appropriate table pointer behind. Then the happy path maps
to the most used operations (non-h series). Of course, in this case, the
non-syllabic stuff still goes through the two tables per char.

Or, instead of returning a simple Option<str> from the tables, I could
also return a "h-series" variant, then use the current dialect to
deal with that. Same with the h-s series. I'm matching anyways on the
Some/None case, it'd just be another case or two.

Today:
- Unify the unambiguous tables into one base map,
- Create "convert from but not to" maps for syl→lat and vice versa
- Put charts on wikipedia page for ease of use -- Skip, too much work.

Maybe I can use the tables in my code to generate HTML tables. Ie. a
simple table with just the short versions, with a toggle for longs,
ng → ŋ, etc. That way the rules can be easily visualized, plus the tables
for nattilik, etc.

The 𑪶 series is sometimes "shr", sometimes "sh", sometimes "š".


===

HTML:

Add field button

Each field has a selector with:
["to_syl", "to_lat", "normalize_syl", "normalize_lat"]

Also, a selector at to of page to select layout:
[ A ]
[ B ]
[ C ]

Or

[A][B]
[C]

Maybe wrap the outputs in folds so they can be minimized.

Default:

[ to_syl ]
[ to_lat ]

===

Eliminate recursive function in converter (WASM doesn't do
TCO really)

===

I like using PHF for its efficiency, it allows me to load a hashmap at runtime without hashing and compiling all the entries. As it stands now, however, there are some pitfalls. Namely, I'm using a system of "cascading hashmaps" which feels inefficient. I would like to avoid this.

Why? The original idea for this hashmap system was TempleOS' symbol structure. In TempleOS, every single symbol in the entire OS is hashed (as I understand it), and if a saught symbol isn't found in the processes' hashmap, the parent is checked, and so on. If this works for every symbol on an OS, why can't it work for a textual system, converting tiny amounts of text, with the only speed requirement being "the user doesn't notice"?

There's really no reason to do it like this. Surely, CSS uses a similar datastructure for its "cascading" rules?

Besides, in reality, I'm going to be allocating and copying a fair bit. Like, I have to lowercase the text I'm processing, which is a whole other issue in itself.

I believe my frustration is with the "impurity" of it all.

If I were to use a regular hashmap, I would update it dynamically based on the ruleset I wanted. For example, replacing the H syllabic, or loading/unloading the "ai" series.

I have an idea that would combine that last idea with the efficiency of PHF, though I'm not sure if it's possible, or even a good idea.

I would create perfect hashes for all the entries at once, ensuring no collisions, and then I'd selectively create rulesets from that big map. For example, I would have the "ai" series hashed, but not include that in the final loaded hashmap if the dialect didn't use the series.

Of course, the duplicate keys thing is an issue. The H series could have multiple results based on dialect. One idea could be to keep the compiled hash, but swap out the returned value. Not sure if that's possible or even practical.

What's Rust actually doing when loading a PHF hashmap? What would my "dynamic but not" hashmaps even look like, how would compiling them work?

It's clear that creating this system would add a lot of complexity with no little upside, so how is that "more pure"? I believe the reason I'm obsessing over this is that I'm anxious about what to do next in the project, particularly how to deal with dialects in the code, and how advanced I want the WASM interface to be.

I think I need to look at the exceptions listed on the Wikipedia chart, and decide what I need to do with each.

---

[o] Nunavik -- "ai" column -- Option
[o] Nunavik -- ᕹ/h series -- H Option
[ ] Nunavut -- ᖅᑲ -> qqa (qq series) -- Unambiguous (I think)
[ ] Nunavik -- ᖓ, ᙵ look different -- Font issue, ignore.

[o] Nunavut -- ᕼ -> H -- H Option
[o] Nunavut (East) -- ᓴ -> s -- *
[o] Nunavut (West) -- ᓴ -> h -- *

[ ] Nattilik -- 𑪺 -> ša -- Unambiguous
[o] Nattilik -- 𑪴 -> ha -- H Option **
[ ] Nattilik -- ᖬ -> řa -- Unambiguous
[o] Nattilik -- ᖓ, ᙵ -> ŋ, ŋŋ -- Option

[ ] Many dialects -- ᖤ -> ła -- Unambiguous
[ ] Aivilik -- ᖯ -> b -- Unambiguous (probably safe enough)

* I don't know what to make of ᓴ, I think I'll have to do some research.

** Is this used differently than the standard Nunavut and Nunavik "H" options?
I want to look into the use of H syllabics, how commona are they.

Added with the Kevin King proposal, will look into it.

For š series, I think shr should be an unambiguous normalization, since it seems common, but less preferred to š.

---

left hand corner could be doubled consonant

qaa qi  qii
qa  q   qu
qq  qai quu
↑↑

ᖄ ᕿ ᖀ
ᖃ ᖅ  ᖁ
ᖅᒃ ᙯ ᖂ
↑↑

Might make more sense, since doubled consonants are often different
than just putting the consonant twice, ie. ᖅ

---

Add & as ł normalization(?) Maybe overzealous.


https://www.itk.ca/projects/inuktut-qaliujaaqpait-converter/
Converts "ᖬ, 𑪴, 𑪺, ᖤ" to "rha, ha, shra, hla". Should there be an option for this? Should this just be a normalization? I think this is the new "standard" for latin letters, but I'm not sure if it's really preferred.

This is the "Qaliujaaqpait the unified system. I think it should be an option, but perhaps not the primary one.

---

As for the normalization (lowercasing) of latin text before conversion:

A naive approach would be to lowercase all the text first, then run the conversions. I don't love this as it requires reallocation, but also, any unconverted characters lose their case, which is not ideal. In reality, I suppose I don't hope a whole lot of latin characters will go through, as I hope to automatically detect Inuktitut words and only convert those, in most cases.

But still, it should keep the case.

What I think I should do is re-engineer the hashmap stuff, so instead of operating on bytes, it operates on characters. If my max byte count is 5, for example, I can check if the characters fit inside those bounds (with my next_jump function), and if they don't, then skip. Actually, I can look at the bounds of the characters, then union those two arrays, ie. If my KEY_LENGTHS are [6, 5, 4, 2], and the bounds of the chars are only [4, 2], then I can skip 5 and 6. Hell, if the next char is 1, I can skip all checks!

If the bounds are ok, I should probably just load those bytes into a register / temp var, then go through and lowercase the chars in place. Caveat: lowercasing in place is not usually done, since there can be different byte lengths. However, for the chars I'm looking for, I don't *think* that's the case. Therefore, I should write my own little lowecasing function, and use tests / prop tests to make SURE the lengths won't change.

That way I can skip a lot more checks, and I don't have to reallocate to do the lowercasing. On the other hand, I'll be doing a lot more lowercasing, and it will probably not be great for cache locality. I don't think this will matter though, the speed will not be noticable, and the main thing is that case is preserved for non-inuk letters.

Speaking of, of course, I will not be using my register loaded values if the non-conversion path is taken, just standard skip and add to str buff.

I mean, I could probably use some soft of intelligent copying to make sure I don't relower the same letters over and over. I guess I should, it will probably just be a little bit of logic.


Should I just use a u64 to put the bytes in? For now, my biggest KEY_LENGTH is 6. I don't think there will be much more added, maybe some normalizations. I can see the byte number reaching 8, possibly with the "Qaliujaaqpait" normalizations. Well no, they're all ascii, and none are 8 long. I think we're safe for now, anyway. I guess in that case, I could make the hashmap work un u64s, instead of u8s. It would make maps.rs less pretty, but more efficient since we'd no longer be pointer chasing. Again, not that it matters.

Ok, Ɫ and ɫ have different byte lengths. I don't think this should matter, just as long as I can fit the normalized stuff in the buffer. I'm not married to a u64, I just don't want to allocate a string. I'll have to keep the original and the buffer's indices separately.

I mean Rust has a u128, I could use that if the u64 doesn't work out. I should just prop test it.

---

First things first, I should separate the dialect options out, as I listed above. I should also have `generate.py` do the "ai" series separately for each map. Might require a lot of refactoring. Perhaps I should investigate this first, before the busywork of making the separate options.

Converting the ai series from syl -> lat should always work, so that could be considered a unidirectional normalization, but the other way around, only if enabled.

Right now, maps.rs looks like:

BASE_TO_SYL
NUNAVIK_TO_SYL
BASE_TO_LAT
NUNAVIK_TO_LAT

I want it to be like:

BASE_TO_SYL
BASE_TO_SYL_AI
BASE_TO_LAT

NUNAVIK_H_TO_SYL
NUNAVIK_H_TO_SYL_AI
NUNAVIK_H_TO_LAT

This makes me think. The Nunavik H series in ambiguous from lat -> syl, because there are multiple H series, but unambiguous in the other direction. Maybe I need a DSL to map out these relationships.

There's also the fact that ideally, I could have a scanning pass that would try and detect the variety, then convert based on that. Ehhhhh.

Maybe like:


	ᐁ	ᐃ	ᐄ	ᐅ	ᐆ	ᐊ	ᐋ	
p	ᐯ	ᐱ	ᐲ	ᐳ	ᐴ	ᐸ	ᐹ	ᑉ
t	ᑌ	ᑎ	ᑏ	ᑐ	ᑑ	ᑕ	ᑖ	ᑦ
k	ᑫ	ᑭ	ᑮ	ᑯ	ᑰ	ᑲ	ᑳ	ᒃ
g	ᒉ	ᒋ	ᒌ	ᒍ	ᒎ	ᒐ	ᒑ	ᒡ
m	ᒣ	ᒥ	ᒦ	ᒧ	ᒨ	ᒪ	ᒫ	ᒻ
n	ᓀ	ᓂ	ᓃ	ᓄ	ᓅ	ᓇ	ᓈ	ᓐ
s	ᓭ	ᓯ	ᓰ	ᓱ	ᓲ	ᓴ	ᓵ	ᔅ
l	ᓓ	ᓕ	ᓖ	ᓗ	ᓘ	ᓚ	ᓛ	ᓪ
j	ᔦ	ᔨ	ᔩ	ᔪ	ᔫ	ᔭ	ᔮ	ᔾ
jj	ᑦᔦ	ᑦᔨ	ᑦᔩ	ᑦᔪ	ᑦᔫ	ᑦᔭ	ᑦᔮ	ᑦᔾ
v	ᕓ	ᕕ	ᕖ	ᕗ	ᕘ	ᕙ	ᕚ	ᕝ
r	ᕂ	ᕆ	ᕇ	ᕈ	ᕉ	ᕋ	ᕌ	ᕐ
q	ᙯ	ᕿ	ᖀ	ᖁ	ᖂ	ᖃ	ᖄ	ᖅ
qq	ᖅᑫ	ᖅᑭ	ᖅᑮ	ᖅᑯ	ᖅᑰ	ᖅᑲ	ᖅᑳ	ᖅᒃ	NUNAVUT
ng	ᙰ	ᖏ	ᖐ	ᖑ	ᖒ	ᖓ	ᖔ	ᖕ	....	AMBIGUOUS_SYL
nng		ᙱ	ᙲ	ᙳ	ᙴ	ᙵ	ᙶ	ᖖ	....	AMBIGUOUS_SYL
ŋ	ᙰ	ᖏ	ᖐ	ᖑ	ᖒ	ᖓ	ᖔ	ᖕ	NATTILIK	AMBIGUOUS_SYL
ŋŋ		ᙱ	ᙲ	ᙳ	ᙴ	ᙵ	ᙶ	ᖖ	NATTILIK	AMBIGUOUS_SYL
ł		ᖠ	ᖡ	ᖢ	ᖣ	ᖤ	ᖥ	ᖦ
b								ᖯ
h								ᕼ	NUNAVUT	AMBIGUOUS_LAT	H
ʼ								ᑊ
ř		ᖨ	ᖩ	ᖪ	ᖫ	ᖬ	ᖭ	ᖮ	NATTILIK
š		𑪶	𑪷	𑪸	𑪹	𑪺	𑪻		NATTILIK
š		ᓯ̵	ᓰ̵	ᓱ̵	ᓲ̵	ᓴ̵	ᓵ̵	ᔅ̵	NATTILIK	NORMALIZE_TO_LAT
h		ᓯˋ	ᓰˋ	ˎᓱ	ˎᓲ	ᓴˏ	ᓵˏ	ᔅ̷	NATTILIK	NORMALIZE_TO_LAT	H
ɫ		ᖠ	ᖡ	ᖢ	ᖣ	ᖤ	ᖥ	ᖦ	NATTILIK	NORMALIZE_TO_LAT
&		ᖠ	ᖡ	ᖢ	ᖣ	ᖤ	ᖥ	ᖦ	NATTILIK
h	ᕴ	ᕵ	ᕶ	ᕷ	ᕸ	ᕹ	ᕺ	ᕻ	NUNAVIK	AMBIGUOUS_LAT	H


What do I really want?

Well, I'd like to be able to generate some cascading hashtables that don't go too deep (3 levels at most, ideally), that convert text into other text directly. I'd like to keep that logic simple, with the possible exception of the aforementioned normalization (ie. lowercasing) code.

For the table, I would love to have one source of truth, from which I can generate all those tables with no other input, as well as some other tables or sets that would allow me to write a function to analyze some text and determine the dialect, the vowels and consonants used, etc. I would love to be able to put some text into an input box on the web, and have it show me the characters used in a table, in a heatmap like display with subscript numbers showing the count of each character, and perhaps a number on each row (series) showing the count of the series in total.

If the text used Nattilik characters, I'd love for it to automatically put the Nattilik table down below it, and show the same heatmap. If it also used Nunavik characters (for some reason), it should do the same.

Why? Well, I want to analyze text from the web, to get a better understanding of what and how characters are actually used. Plus, I want the converter to have some basic heuristics on what latin or syllabic symbols to use, if there's ambiguity.

---

How does directly converting from text to text limit my ability to analyze?

Converting without analyzing and converting while analyzing remove the main benefits of analysis. In both situations, we'd be ignoring hints about analysis (ie. this text is from Nunavik). So analysis should come beforehand.

Should unambiguous text always be converted, or only converted if that specific dialect is being converted from? The former, all unambiguous text (lat to syl or vice versa) should be converted, with the possible exception of b → ᖯ.

Should text be passed over multiple times to settle options? This is a tricky one, consider the case of ᐁ vs ᐊᐃ. It's tempting to always convert to the ↓ai column, then go through and reconvert to the a+i ᐊᐃ variants, because on the second run through, we can just use a table with all the ↓ai variants and convert those, where if we did it in the original run through, we'd have to make certain that the ↓ai variants we're putting in front of each table correspond to those values.

For example, "hai" should be "ᕼᐁ" if we're using standard Nunavut syllabics and the ai column (an odd choice nowadays, but common pre 1970s if I'm not mistaken). So we can't convert it to "ᕴ", because that's a Nunavik thing, and we're not using the Nunivik table. So, if we did want the Nunavik table and the ai column, we'd need to do:

Nunavik_ai → Nunavik → Base_ai → Base

Or something like that.

But, I think it's worth it. There's no need to reallocate and rerun over all the entries every time, plus there'd still be that big "ai" series table.

It would be nice to have an in-place normalization function, too, that doesn't to syl -> lat -> syl, since that could lose some info. Ie. convert ᓰ̵ to its proper Unicode equivelent, 𑪷, without going through latin.

---

	ᐁ	ᐃ	ᐄ	ᐅ	ᐆ	ᐊ	ᐋ	
p	ᐯ	ᐱ	ᐲ	ᐳ	ᐴ	ᐸ	ᐹ	ᑉ
t	ᑌ	ᑎ	ᑏ	ᑐ	ᑑ	ᑕ	ᑖ	ᑦ
k	ᑫ	ᑭ	ᑮ	ᑯ	ᑰ	ᑲ	ᑳ	ᒃ
g	ᒉ	ᒋ	ᒌ	ᒍ	ᒎ	ᒐ	ᒑ	ᒡ
m	ᒣ	ᒥ	ᒦ	ᒧ	ᒨ	ᒪ	ᒫ	ᒻ
n	ᓀ	ᓂ	ᓃ	ᓄ	ᓅ	ᓇ	ᓈ	ᓐ
s	ᓭ	ᓯ	ᓰ	ᓱ	ᓲ	ᓴ	ᓵ	ᔅ
l	ᓓ	ᓕ	ᓖ	ᓗ	ᓘ	ᓚ	ᓛ	ᓪ
j	ᔦ	ᔨ	ᔩ	ᔪ	ᔫ	ᔭ	ᔮ	ᔾ
jj	ᑦᔦ	ᑦᔨ	ᑦᔩ	ᑦᔪ	ᑦᔫ	ᑦᔭ	ᑦᔮ	ᑦᔾ
v	ᕓ	ᕕ	ᕖ	ᕗ	ᕘ	ᕙ	ᕚ	ᕝ
r	ᕂ	ᕆ	ᕇ	ᕈ	ᕉ	ᕋ	ᕌ	ᕐ
q	ᙯ	ᕿ	ᖀ	ᖁ	ᖂ	ᖃ	ᖄ	ᖅ
qq	ᖅᑫ	ᖅᑭ	ᖅᑮ	ᖅᑯ	ᖅᑰ	ᖅᑲ	ᖅᑳ	ᖅᒃ	NUNAVUT
ng	ᙰ	ᖏ	ᖐ	ᖑ	ᖒ	ᖓ	ᖔ	ᖕ	
nng		ᙱ	ᙲ	ᙳ	ᙴ	ᙵ	ᙶ	ᖖ	
ŋ	ᙰ	ᖏ	ᖐ	ᖑ	ᖒ	ᖓ	ᖔ	ᖕ	NATTILIK
ŋŋ		ᙱ	ᙲ	ᙳ	ᙴ	ᙵ	ᙶ	ᖖ	NATTILIK
ł		ᖠ	ᖡ	ᖢ	ᖣ	ᖤ	ᖥ	ᖦ
b								ᖯ	AIVILIK
h								ᕼ	NUNAVUT
ʼ								ᑊ
ř		ᖨ	ᖩ	ᖪ	ᖫ	ᖬ	ᖭ	ᖮ	NATTILIK
š		𑪶	𑪷	𑪸	𑪹	𑪺	𑪻		NATTILIK
š		ᓯ̵	ᓰ̵	ᓱ̵	ᓲ̵	ᓴ̵	ᓵ̵	ᔅ̵	NATTILIK
h		ᓯˋ	ᓰˋ	ˎᓱ	ˎᓲ	ᓴˏ	ᓵˏ	ᔅ̷	NATTILIK	NORMALIZE_SYL
ɫ		ᖠ	ᖡ	ᖢ	ᖣ	ᖤ	ᖥ	ᖦ	NATTILIK	NORMALIZE_LAT
&		ᖠ	ᖡ	ᖢ	ᖣ	ᖤ	ᖥ	ᖦ	NATTILIK	NORMALIZE_LAT
h	ᕴ	ᕵ	ᕶ	ᕷ	ᕸ	ᕹ	ᕺ	ᕻ	NUNAVIK

^^^ Add Nattilik 𑪴 ha series
Also the chart entry for ᓴ is wrong.

===

May 10th, 2024

I now like the idea of a small DSL for the syllabics stuff, especially because it could aid greatly in the autogenerating in other tooling. For example, I'd like to be able to simply detect the language used on a webpage. Ideally, I will be downloading a huge dataset of all Inuktitut websites and categorizing them into different dialects, and possibly analyzing the characters they use such as non-standard (non-normalized) forms of characters, etc.

I would be analyzing Syllabic and Latin text and considering them mostly equivalent.

This tool should probably be written in Python since I have no need for performance and it would be nice to integrate more tightly with the webscraping tooling. For example, this could open up the possibility of following links and continuing to download pages if they contain a lot of Inuktitut (as determined by the code).

I'd like the Rust code to be able to detect dialect as well, though it doesn't have to analyze the characters as much as the Python would, just normalize and detect language. Perhaps these two systems should be integrated after all, since they're so similar. That's yet to be seen.

---

So, should there be an intermediate representation in the Rust code? For direct conversion it hardly seems necessary, but for dialect analysis it seems natural, right? Let's think.

If we encounter 𑪶 in the code, is it different than the Latin "shi"? Yes, it's in syllabics. Does that matter for our purposes? Well, we want to:

1. Detect the dialect, and
2. Convert it to syllabics or vice versa

If we store the intermediate representation (either as a string, "shi", or some token format, [S, H, I]), but we don't store whether it was in Latin or Syllabics in the first place, that's useless for goal 2.

We could say that both 𑪶 and "shi" are associated with Nattilik. This duplication will not happen really that often, and it's all in pre-generated tables, so we could be a fraction more efficient and have less logic in the code.

Another issue is normalization. Should we normalize, say, syllabics by going:
syl → lat → syl, or could we go
syl → syl

The latter would require more tables to be generated. The real question is whether these two would produce the same results. Logically I think so but my intuition is skeptical. I think this would be a good area for some property tests.

---

Back to the original question, should there be an intermediate representation? I'm leaning towards yes (maybe), but I don't think it has to be in some odd token format, I think it can just be a string. If it's a Latin string with what I'll call the "de-fato" Latin alphabet (ie. with š and ł), then there should be no amiguity except the s/h thing.

As far as I know, dialects that use H in syllabics only are going to use one type. Nattilik uses 𑪴, Nunavut uses ᕼᐊ, and Nunavik uses ᕵ (ostensibly, I don't believe it's ever used in practice). So that's not an issue for conversion to or from Syllabics, as there's no ambiguity in practice.

The only ambiguity in Latin, as far as I can tell, is that ᓯ means "sa" in some dialects and "ha" in others. Since the dialects that use it for "ha" don't use "sa" (it's shifted phonologically), we could represent it in Latin as "sa" always and convert when required for the "ha" dialects. So our "intermediate representation" could, I believe, be a regular Latin string with the ha-sa ambiguity eliminated by way of always using sa. If I ever extend this to Ojibwe and Cree, I might want a better system, but chances are I'd have to gut most of the logic anyway, so that shouldn't be a concern.

---

Let's examine the analysis requirements for a complex part of this project: the web analyzer. Ideally, I'd like to create a program that I can point at a webpage with Inuktitut on it, and it will find and analyze all the text on that page. It shouldn't be operating on the raw HTML, but rather what's actually presented on the page. Perhaps Selenium will be required if there's a lot of JS happenings, plus Selenium looks good on a resume.

Regardless, the tool should find all the Inuktitut text. It can then report on the percentantage of the page is in Inuktitut. It should go through the text and find the dialect, based primarily on what characters are present in each word. It should be able to report normalizations.


[ WORD | NORMALIZED_WORD | PRESUMED_DIALECT | URL ]
[ ᐃᒪᒃ  | imak            | Nunavik          | https://nunavik-ice.com/iu/c/tag/%E1%91%95%E1%95%86%E1]

I would also like something like above in SQLite, if I have the space for it.

[ URL | INUKTITUT_PERCENTAGE | PRIMARY_PRESUMED_DIALECT | PRIMARY_SCRIPT ]
[ https://iu.wikipedia.org | 90 | Nunavik | syllabic ]

Perhaps that too.

---

Another thing I'd like is some property-based tests that compare the output of my tool with the output of other tools. Perhaps I would require Selenium, but I think I could also download the JS client-side in most cases and test against that (without checking it into the source tree).

---

Back to scraping. I think I should be able to point it at a page and have it do some magic. Namely, it should see if the Inuktitut level on the page is above ±5%, and if so, follow the links on the page that haven't been seen before. Then, it keeps doing that. That way, non-Inuktitut pages don't get exponentially scraped, while the entire Inuktitut web should be fairly easy to cover. Saved pages need to be cached. I think I'll only focus on HTML pages, though PDFs are tempting for sure.


WORD(str)
WORD_COUNT(int)
NORMALIZED_WORD(str)
PRESUMED_DIALECT(enum)
RESOURCE_TYPE(enum) WEB | PDF
URL(str)

---

If possible, I'd like to be able to look at things like consonant clusters and such for the language classifications.

---

Maybe for the English detection stuff, I'll do it all in Rust instead of two implementations in Rust and Python. I can add a list of all English words and subtract Inuktitut words from a dictionary or something from the list (I've done this before). Since I don't want to use all English words in my app, I can just put that part behind a feature flag. If I want accurate, I can use the word list to help, if I want fast, I use no- or fewer words. Same with French.


=== June 13th, 2024 ===


Getting back into things, I think a really important step is creating a program that can run through given text and analyze it. Firstly, it needs to be able to detect (likely) Inuktitut text, be in it latin or syllabic form. Inuktitut meaning all Canadian Inuit dialects aside from Labridormiut. It doesn't need to copy the text per se, it can point to it, but it should store some attributes about it (right?). If text is syllabic, that should be known, and if text beside it is in latin script, that should be known as well, separately.

>> The [Nunavik] dialect ([Nunavimmiutitut], [ᓄᓇᕕᒻᒥᐅᑎᑐᑦ]) is relatively close to the South Baffin dialect, but not identical. Because of the political and physical boundary between [Nunavik] and [Nunavut, Nunavik] has separate government and educational institutions from those in the rest of the [Inuktitut]-speaking world, resulting in a growing standardization of the local dialect as something separate from other forms of Inuktitut. In the [Nunavik] dialect, [Inuktitut] is called ` ([ᐃᓄᑦᑎᑐᑦ]). This dialect is also sometimes called [Tarramiutut] or [Taqramiutut] ([ᑕᕐᕋᒥᐅᑐᑦ] or [ᑕᖅᕐᕋᒥᐅᑐᑦ]).

Nunavik
Nunavimmiutitut
ᓄᓇᕕᒻᒥᐅᑎᑐᑦ
Nunavik
Nunavut, Nunavik
Inuktitut
Nunavik
Inuktitut
ᐃᓄᑦᑎᑐᑦ
Tarramiutut
Taqramiutut
ᑕᕐᕋᒥᐅᑐᑦ
ᑕᖅᕐᕋᒥᐅᑐᑦ

Must I presume I dialect before normalizing the word? I don't think so. As discussed above, I can boil everything down to a base latin, and the only ambiguity is whether the 's' remains an 's' or transforms into an 'h'. That is irrelevant in the normalization step. I can analyze all the strings later, as their original will be saved. That being said, there's no point in putting it off. The tokens being matched will have been generated from the Table of Truth (as I'll dub it), and so characters that are associated with a specific dialect can be reported. But what to do if a word contains characters from multiple dialects? Let's consider the following:

Text is split into 'chunks'. These chunks can contain other chunks recursively. Let's assume for now that the chunking only applies to text seen to be likely Inuktitut:

>> The [Nunavik] dialect ([Nunavimmiutitut, ᓄᓇᕕᒻᒥᐅᑎᑐᑦ])
A: Nunavik
B: Nunavimmiutitut, ᓄᓇᕕᒻᒥᐅᑎᑐᑦ

A.chunks:
>> [Nu][na][vi][k]

A.A:
>> [Nu]

A.A.script
>> [latin]

A.script
>> [latin]


B.script
>> [latin, syllabic]

B.chunks:
>> [Nunavimmiutitut][ᓄᓇᕕᒻᒥᐅᑎᑐᑦ]

B.A.script
>> [latin]

B.B.dialect
>> [standard]


[ᓄᖅᑲᕆᑦ] (nuqqarit)

A.script
>> [syllabics]

A.dialect
>> [standard, nunavut]

A.A.dialect (ᓄ)
>> [standard]

A.B.dialect (ᖅᑲ)
>> [nunavut]

Technically, in the last example, the Nunavut sequence ᖅᑲ can be divided into 'ᖅ' and 'ᑲ' (it's two characters). But consider this, the sequence "qqa" which it represents could techncially be split into 'q', 'q', and 'a'. This is useless. When dealing with both the syllabics and the latin letters, we're dividing the tokens into their longest possible counterparts.

In any case, the point of this "chunking" is not that we need the finest possible granularity on everything, but the opposite. Getting this granularity should make it simpler to group larger segments of text together. For example, if there's a paragraph where many of the words contain some sequences that are associated with Nattilik and no other dialects, and there exist no other dialectal sequences except the standard ones, the Nattilik can be bubbled up to the larger chunk.

Furthermore, this system could potentially help with differentiating non-Inuktitut words. Many English and French words fit the Inuktitut spelling system well, but are not Inuktitut. These can be filtered out by way of dictionaries (or hashtables). In most cases, I think these hashtables could be quite small and fit into the tokenization process quite well. Larger tables could potentially be used and toggled between with a compile-time feature, if needed.

In any case, when these filtered words are encountered, they can be made a chunk in themselves, marker as English or French or whatever. This could also be used for names. Many names have Inuktitut spellings, but the latin spellings resemble the English names. For example, a search for ᒐᕕᓐ (Gavin) returns results on the Internet, but my spelling of "Gaven" wouldn't work.


On Wikipedia, there's a:

>> ᔮᓐ ᒦᐆᕐ -- jaan miiuur -- John Muir

Of course not all names are going to be easily transliteratable or even consistantly spelled in Inuktitut, but I think it's worth having a list of such names and their transliterations and chunking them differently.

The idea is, of course, that we're already working from the smallest atoms. In the case of Inuktitut words, it's the things that can be chunked into syllabic characters, like "i" and "nngaa". In the case of English words, it's the whole words, as there's no need to go further. Same with names. In this way, working from the ground up, tokenizing in the smallest required chunks as we go, we can build up a series of granular chunks that can be combined in another pass. 1000 English words can be combined into one English chunk, 1000 syllabic inuktitut words can be combined in another. Neutral chunks containing punctutation and numbers can be combined with their neighbours (if need be).

In the practical sense, we're tokenizing these things by way of grabbing the largest possible tokens from the beginning of the string and consuming it. This will require a bit of work, namely, we're likely to have to copy the start of the string up to the largest possible token size, then lowercase it. Ok. That presents another issue, namely that the numbers of bytes and potentially(?) even the number of characters have chnaged, so that presents an issue. Say we want to keep the English word "London", but now it's lowercased as "london", so we must keep track of where we are in the original string. 'L' and 'l' are the same byte length in upper and lowercase, but this isn't the case for all, like Ɫ and ɫ (see above).

So that's a pain. Another pain: What if we don't find a filtered word from English or French, and we start tokenizing it. What do we do if the end of the word isn't Inuktitut, do we un-tokenize it? Like, "makademashkikiwaaboo", the [maka-] could be Inuktitut, but clearly the rest isn't. Do we really only want to operate on tokens?

Perhaps we have a main buffer for all the bottom-level chunks, then a temporary buffer for things that are not yet certain. So:

>> John is an Inuk, he speaks ᐃᓄᒃᑎᑐᑦ and drinks makademashkikiwaaboo

name, [John]
english, [ is an ]
inuktitut, [I][nu][k]
english, [, he speaks ]
inuktitut, [ᐃ][ᓄ][ᒃ][ᑎ][ᑐ][ᑦ]
english, [ and drinks ]
non_inuktitut, [makademashkikiwaaboo]

But when tokenizing the last word, the temporary buffer is:
inuk [ma]
inuk [ka]
non_inuk (restart tokenization)

What is a word boundary for Inuktitut? I would say whitespace, end of string, punctuation. Not numbers, a lot of times numbers can be directly attached to the syllabics, and sometimes dashes are used like "55-ᖓᓂ" or "55ᖓᓂ". Dashes are weird, because I believe they can be used inside Inuktitut words (rarely), but can also connect Inuktitut words to English in an English context. Consider the example that I took from above:

>> The Inuktitut-speaking world

This is clearly English, but who cares? The word "Inuktitut" is in Inuktitut for our purposes. As such, I think dashes should also be considered neutreal. If there's two Inuktitut words on either side of it, the chunks can be chunked together as Inuktitut text. If not, then we can do wheatever else we need.

So, who owns punctuation? I feel that English and non-Inuktitut text should have a stronger "bond" to it. If there's a lot of English and a few Inuk words, then the punctuation can belong to the English chunks on a higher level. For the most part, it shouldn't matter, since the punctuation and neutral characters are never going to converted anyways. If I want to use the tool to count the percentage of Inuktitut on the page, I think it should count internal punctuation for sure, but if there's other languages surrounding it, it can probably belong to those other languages. We'll see.

Numbers can be considered neutral. Perhaps they'll require their own category.

I think it could work well with:

inuktitut
non_inuktitut
neutral
name

I suppose whitespace too. Perhaps that's "neutral".

The Inuktitut wikipedia could be good for testing. The transliteration is automatic (and flawed), but generally, it would be good to test that each page in both forms normalized to the same thing in my tool.

The automatic transliteration gives such nightmares as the following:

>> ᑦᐦe ᐦᐃᒡᐦ ᐊᕐcᑎc ᕐeᓪocᐊᑎoᓐ: ᐊ ᕐeᑉoᕐᑦ oᓐ ᑦᐦe 1953–55 ᕐeᓪocᐊᑎoᓐ

What am I supposed to do with that? I suppose my tool would just ignore most of it as non_inuktitut by default, which might not be the worst option.

So!

I want a few TSVs.

- The Table of Truth (ToT), which has all the syllabics, syllables, and information for different tooling.

- The name table, which has English and French names and their equivelents, perhaps a column for sources.

- A list of all English words.

- A list of all French words.

These will all be combined at runtime. In practice, we don't need *all* English and French words in the hashtable. It could be useful for analysis on the web, but on iOS, we could have a compile flag that only takes the first 1000 or so. Actually, we only need the words that could be construed as Inuktitut by the system but are uniquivically not. I've basically created this filter before, it was quite simple. So no need to store all words. Maybe could help with analysis though for the webpages and whatnot.

Let's look at tokenizing this paragraph:

>> Canadian Inuit live throughout most of Northern Canada in the territory of Nunavut, Nunavik in the northern third of Quebec, the Nunatsiavut in Labrador, and in various parts of the Northwest Territories and Yukon (traditionally), particularly around the Arctic Ocean, in the Inuvialuit Settlement Region.

Let's say the longest word in our hashtable is "incomprehensibilities", an English word. In practice it would likely be shorter because we might only include words that match potential Inuktitut words (not sure yet, as discussed above).

It's 21 characters, but we don't need to run the tokenizer against the first 21 bytes. We have to do some processing first anyway. The first word in the string is "Canadian", 8 bytes. We're not going to tokenize things with spaces in them, so we only have to tokenize that far. Also, we don't want to tokenize on bytes, but on characters.

**Sidenote: as I write this, the tokenizer for the converter does work on bytes, not characters. The input string comes from a str, so with a clever "next" function, I ensure that we never land inside a byte and crash things. But I will remove this soon, which will clean up the Rust file with the maps considerably. More importantly, we're already operating on characters and have to do string work anyway, so there's no reason to operate on bytes.

Also, we need to copy the string "Canadian" to a new buffer (unfortunately) in order to decapitalize it.

**Sidenote on lowercasing: So the lowercasing shouldn't be as bad as I thought. We actually don't really care about the ability to lowercase every script's letters on Earth, we just care about a small subset of letters. So, all of A-Z, as well as the few letters like Ɫ and ɫ. They have different byte lengths, sure, but they are the same number of characters. There's only a few of these (ł, ŋ, š, ř), so we can guarantee the number of chars will always be the same. We'll just write our own tiny lowercasing function.

>> [Canadian][ ][I][nu][i][t][ ][live][ ][throughout][ ][most][ ][of][ ][Northern][ ][Canada][ ][in][ ][the][ ][territory][ ][of][ ][Nu][na][vu][t][, ][Nu][na][vi][k][ ][in][ ][the][ ][northern][ ][third][ ][of][ ][Quebec][, ][the][ ][Nu][na][t][s][i][a][vu][t][ ][in][ ][Labrador][, ][and][ ][in][ ][various][ ][parts][ ][of][ ][the][ ][Northwest][ ][Territories][ ][and][ ][Yukon][ (][traditionally][), ][particularly][ ][around][ ][the][ ][Arctic][ ][Ocean][, ][in][ ][the][ ][I][nu][vi][a][lu][i][t][ ][Settlement][ ][Region][.]

The above would be the initial tokanization (chunking), without tagging. It should be obvious what each chunk is here. Realistically, the neutral elements between the English words might be combined even before a second pass to save space and clean up output, as they don't contain anything meaningful. A second layer of chunking would be:

>> [Canadian ][Inuit][ live throughout most of Northern Canada in the territory of ][Nunavut, Nunavik][ in the northern third of Quebec, the ][Nunatsiavut][ in Labrador, and in various parts of the Northwest Territories and Yukon (traditionally), particularly around the Arctic Ocean, in the ][Inuvialuit][ Settlement Region.]

The higher level chunking won't really combine the lower chunks as they contain useful information (see chunk justification above). This will be true for Inuktitut, but should it be true for English, French, and other non-inuktitut? I think so, because it would be nice to be able to see the language of the words, or if the language is unclear, why they were tokenized as non Inuktitut.

My example tokenization is wrong as 2nd-order though. For Inuktitut, the second-order chunks should definitely be on word boundaries.

So, English words are tokenized as-is in the 1st order chunking. This would be true, I suppose, whether they existed in the dictionary or not. If a word doesn't match any of the tokenization patterns in the list, it's considered non-inuktitut.

We've been treating the nth-orders as if they mean nothing, but they do. 1st order, for Inuktitut, are the individual atoms that can be represented as syllabics. Second order are the words. Third order are the connected phrases. Fourth? What is there are two sentences in Inuktitut beside each other in different dialects, or different scripts? What then? Script boundaries are easy, but dialect boundaries are not, as most words in most dialects share most of the same characters. Looking up individual words and morphemes in a dictionary is beyond the abstraction of this tool. Perhaps we just add them together:

"This paragraph contains both Nunavut and Nunavik dialect", or simply

[nunavut, nunavik]

It's likely to happen. For example, a sentence that says "In Nunavut we say X but in Nunavik they say Y".

But we're not operating on the paragraph level, right? So what if an entire page is in Nunavik dialect, but includes a single character of Nunavut dialect? The whole page is considered both? I suppose if I'm storing words in a database, I can look at the chunk attributes and store only the known dialect, but still.

Also, this makes me think. If we're chunking and not caring about the paragraphs and whatnot, this is going to make integration with HTML a huge pain. You can't start a <div> in one <p> and end it in another.

Well, for working with the Internet, I'm going to be grabbing the text from <p> and <h1>, etc. tags anyway, so the content will arrive like that and potentially be pushed out in that format too. So no issue.

Pseudocode:


struct Chunk {
    text: &str,    
    children: Vec<Chunk>,
    token: Token,
}

struct Inuktitut {
    normalized: String,
    dialects: Vec<Dialects>,
    scripts: Vec<Scripts>
}

enum OtherLanguage {
    English,
    French,
    Unknown
}

struct Other {
    language: OtherLanguage,
}

struct Name {
    normalized: String,
}

enum Token {
    Inuktitut(Inuktitut),
    Other(Other),
    Neutral,
}


--- ChatGPT made it like this: ---


#[derive(Debug, Clone)]
struct Chunk {
    text: String,    
    children: Vec<Chunk>,
    token: Token,
}

#[derive(Debug, Clone)]
struct Inuktitut {
    normalized: String,
    dialects: Vec<Dialect>,
    scripts: Vec<Script>,
}

#[derive(Debug, Clone)]
enum OtherLanguage {
    English,
    French,
    Unknown,
}

#[derive(Debug, Clone)]
struct Other {
    language: OtherLanguage,
}

#[derive(Debug, Clone)]
struct Name {
    normalized: String,
}

#[derive(Debug, Clone)]
enum Token {
    Inuktitut(Inuktitut),
    Other(Other),
    Neutral,
}

It converted my &strs to Strings, which really is fair. I would love to avoid all this allocation. In reality though, I do want to save the original words, right? If I save them in a DB, then I'm going to have to copy them. If I want to plop them back on the original page as-is, like the English words that would remain unchanged, I don't really need to save them though, especially each word inidividually. I can just copy them when I'm rewriting the string. Let's think about this.

If I never copied the original string (except for the tokenization search stuff), how would that affect my design? It would do a hell of a lot less allocation for one. I'd have to make sure to keep the original string around. Is that an issue? I mean this is all fairly functional, straight-forward code. I don't think that'd be an issue.


[I][nu][i][t]

I, i, standard, latin, []
nu, nu, standard, latin, []
i, i, standard, latin, []
t, t, standard, latin, []

[Inuit]
Inuit, inuit, standard, latin, [I, nu, i, t] <- list of child nodes.
^^^^^     ^^
Original  Should be stored as String or list of enums? Probably string.


Should Chunk include more information like Inuktitut percentage? Seems wasteful for lower-order chunks, but useful for the higher order ones. Perhaps each chunk should have the number of characters included in it, and the percentages can be worked out by looking at the nodes near the top of the tree. Of course, should neutral chaarcters be counted? I think so, if they're internal to a given language, they're part of that count. Still, it seems imperfect. But for proactical purposes it should work.

Actually, I can just count the characters of the chunk. I could even just count bytes if I'm lazy. The counts don't give a lot of information anyway, since syllabics have less characters than latin, for example.

** Names category can include Inuit names that were written in a different system. I think the spreadsheet should have a comments / sources column.

So how do I really want to do this tree? Will it be a true tree with one parent node? Thinking of a web page, I'm going to probably be grabbing all the disparate pieces of text from the various tags. In the case where I want to analyse the text to see if the page contains any Inuktitut (or like more than 5%), then having a root node that has the language info would be useful. That's not really reflected in my current design though.

In the case where I want to convert all the text on the page to syllabics or latin, I don't need a whole tree, just the inuktitut and non-inuktut text so I can convert and copy respectively.

In the case where I'm analyzing the code and showing the analysis in a web browser interactively, that's where I really want a tree. But my tree and the DOM's tree are different. I want to plop the old text back into the DOM all marked up with my annotations and background colours and hover tooltips. How do I track what goes where? Does my root node do anything for me? Should I keep the DOM info in my tree even if the end goal is not to use this primarily for HTML inspection?

I suppose I don't have to put the whole web page in my tool in that case, the top of the tree could be the text in the <p> tag or whatever.

Sure. Then if I want, I can collect all those disparate processed groups of text together in a vec (ie. a vec of chunks) and I have exactly what I need to make a root node. -Ish. I mean, <p> tags can contain <a> tags and whatnot, I have to think of how I'm going to do this. Strip them? I suppose I could recursively follow the HTML tree and work that way. This Regex extension calls certain elements "prose elements", maybe I could study it.

https://github.com/Mohd-PH/RegexSearch/blob/master/src/content_scripts/page_search.js

Then I'm dealing with two trees. Let's think about this:

h1
p
p {text} a a

Here, the h1 and two p tags are siblings. One p tag contains two a tags. So, I can simply work with the chunks as normal for each, replacing the text in each element as need be, and recurse lower if needed. There can be text beside the a tags, or parallel elemtents, so I convert it and don't recurse.

So what do the chunks look like? Normally, the tree of chunks is based off of the recursive nature of the parsing. Here, they'd also be working off the recursive nature of the DOM.

      / <h1>chunk</h1>
root +- <p>chunk</p>
      \ <p>chunk<a>chunk</a><a>chunk</a>

For the DOM stuff, we just process it before making the root node. If we still want a root node, we can just treat these as normal chunks that are parallel to each other or contain each other. We just care about the text in the chunks, and the tags are irrelevant to us. Some parallel chunks therefore will be beside each other when they would've been merged before ona. higher order, so we can merge them if we wish, though it won't really matter since we'll now be on word boundaries and whatnot. Probably would be fore the best so we can get the text from things like <a> tags beside the text like if we were reading it.

This is important, I think. And I think I worked it out pretty well. But this is not the most important thing. The most important thing is to get this working for a paragraph so I can copy and paste some code in and see it working. All this DOM stuff can come later.


** Test: make sure all names in normalized Inuktitut are valid Inuktitut.

** Thought: The new Qaliujaaqpait (latin) syllabics should be tagged perhaps, as well as other normalizations.

Looks like Makivvik was one of the organizations behind the new latin syllabics. Perhaps I should consider using that as the default latin script.

** Test: make sure the entire syllabic version of the Bible can be transliterated. Especially characters like ᑍ which are rare outside of religious texts.

Some names can be found with the side-by-side Bible. I should keep in mind that these are in the religious form mentioned above. Perhaps I could have another column in the spreadsheet for potential religious names.
 
https://www.bible.com/bible/455/1SA.20.ABM?parallel=1

The DOM stuff bothers me because I'd be making a full tree functionally, but would be potentially editing the HTML imperatively. Why would I need a full tree? I suppose to get things like the Inuktitut percentage on the page.

=== June 15th, 2024 ===

** It looks like Nattilik uses 'ᕠ'? Wikipedia even says one of the names for the dialect is "Natchilingmiutut (ᓇᑦᕠᓕᖕᒥᐅᑐᑦ)". I first came across this on this page:

https://nac-cna.ca/fr/stories/story/the-breathing-hole-an-unforgettable-journey-through-storytelling-and-puppet

That's why it's important to me to have this analysis tool, to see which syllabics are used in practice and how.

See this: https://hadlariconsulting.com/ikajuqtigiit-society

^^ There's an open source (MacOS) keyboard too, perhaps I could get in contact with the authors and see if they have any useful knowledge.


===
June 18th, 2024

NAME        INUKTITUT  NOTES_AND_SOURCES
Akeeagok    ᐊᕿᐊᕈᖅ     See English & Inuktitut versions of this article: https://nunatsiaq.com/stories/article/canada-post-does-not-provide-free-shipping-corporation-says-of-closed-amazon-loophole/

I think this is a good format, though I might need another column for versions that use these ᑍ. Of course there might be multiple versions of a name in either direction, I'll have to think about that. I think it'd be nice to be able to add names in this format, then run a script to alphabetize the TSV/spreadsheet and convert the syllabics into normalized latin form ('aqiaruq', in this case).

-

So, what do I need from the analysis tool? I would like to be able to find all the Inuktitut text on a webpage. This would be helpful for a few things: Firstly, I'd like to be able to convert Inuktitut with very basic conversion tools, and knowing the dialect and script ahead of time would be a big help. I would also like to be able to analyze the text on the page.

What percentage is Inuktitut, what percentage English, what percentage French? Moreover, I'd like to know the specific characters used in the Inuktitut text. Am I missing any? Are some characters very common, are some uncommon? Beyond that, I'd like to see patterns in the normalized forms. For example, are the N-series syllabics used more than the R-series, and how much more or less? What about the A-rotation? How common are long vowels?

On the "breathing hole" article linked above, there's words like "ᐃᓄᒃᑑᓕᖅᑎᑦᕠ'ᓗᖓ", with apostrophes in the middle. I wasn't aware that was acceptable in Inuktitut. I'd like to store the Inuktitut words and a version of the words with all additional non-whitespace characters. So if we had:

>> ᐃᓄᒃᑑᓕᖅᑎᑦᕠ'ᓗᖓ...

[ᐃᓄᒃᑑᓕᖅᑎᑦᕠ]         Original word
[ᐃᓄᒃᑑᓕᖅᑎᑦᕠ'ᓗᖓ...]  Original word with connected characters

That way if there's something missing in my grabbing of words, I can figure out and tune the parser.

One issue is that I previously didn't realize ᕠ was in any Inuktitut dialects. As it stands, my parser would hypothetically have skipped over this word as 'non-Inuktitut', which would mean I lost valuable information. What to do about this?

I don't think this needs to be overly complex. Perhaps the latin case can be ignored completely. I'd like my analyzer to tell me if there are syllabics in my text that aren't in my table.

I'd like to be able to use the tool to scrape pages with Inuktitut text and crawl additional pages.

What's preventing me from moving forward?

I'd like to understand the process and problems fully. So, the analysis tool will always be run when converting text, for example, to determine the correct dialect and, more importantly, for normalization. The normalization should be done in the analysis step.

I'll ignore normalizations for now and start on table generation for the analysis tool.

Possible funding sources from the website link above:

>> The Ikajuqtigiit Society would like to thank the following organizations for their funding support that has made all this possible.
- Department of Culture and Heritage, Government of Nunavut 
- Kitikmeot Inuit Association
- Department of Economic Development, Government of Nunavut, and the Canada Council for the Arts  (for funds to assist in production of the CD shown above, Songs from the Natchilingmiut)  
- Nunavut Literacy Association


(C, V, L)
(Consonant, Vowel, Length)


>> [By ][[ᑕ][ᐃ][ᕕ][ᑦ]] [ᓛ][ᕼ][ᐄ][ᑦ]]

>> [For the English version of this story, please see ][[Pa][n][g][ni][r][tu][ng]]][ pilot lands at home for the first time.]

>> [[ᐅᓇ] [ᔫᓴᕝ] [ᐋᑯᓗᔾᔪᒃ] [ᒥᓐᓂᐊᓕᕋᒥ] [ᐸᓐᓂᕐᑑᒧ] [ᐊᒻᒪ] [ᖃᖓᑕᓲᓂ] [ᐃᖏᕐᕋᑎᓕᕐᒪᒍ] [ᖃᖓᑕᓲᒃᑯᕕᒃᒧᑦ] [ᑕᑯᓚᐅᕐᓯᒪᔪᖅ] [ᐃᒪᓐᓇ]: [ᐃᒡᓘ ᐃᓗᐊ] [ᐊᒻᒪ] [ᓯᓚᑖ ᐃᓄᖕᓂ] [ᑕᑖᓚᐅᕐᓯᒪᔪᖅ] [ᑖᔅᓱᒥᖓ] [ᑐᙵᓱᒃᑎᑦᑎᔭᖅᑐᖅᑐᓂᒃ].]

- Smallest unit: Syllabic unit
- Larger unit: Word
- Larger unit: Chunk (it might be a paragraph, sentence fragment, etc. Contains Inuktitut words and neutral characters like spaces, periods, numbers, etc.)


enum SyllabicLength {
    Short,
    Long,
};

struct SyllabicUnit {
    consonant: char,
    vowel: char,
    length: SyllabicLength,
    original: String
};


There are large parts missing, but when I try to discover and fill the gaps, I feel like I fall through the large gaps in a rocky seawall, and I can't see much of anything at all.

What do I want from this analysis? What can it give me? What is wastage to store? A word I suppose can be stored simply as the Syllabic units. The summation of the Consonant, Vowel, and Length fields will render a normalized value, either as a string or as-is, and the original fields can be summated to create the original string.

But when do I analyze? In this case, I'm normalizing things on-the-fly, I suppose, and so that part is done. But what of the analysis of the syllabics? For example, "this syllabic is an H from Nattalik". I'm keeping the original string around to sort out later. Why? Can I not say it's Nattilik and be done with it, recreating it in the same way as the normalized value, but applying an extra layer of "this is how to interpret this normalized value" on top? Perhaps not, consider the case of ˎᓲ and 𑪹. The "normalization" process isn't just about turning syllabics and latin into a base form. I should store these values that are normalized so I know that they were used.

But when? For the web, I would love to scrape and store the pages, but how? Do I store the HTML, or just the text, or just the Inuktitut text. And then, how do I store the text, as sentences? As words? As disconnected Syllabic Units?

I suppose storing the HTML of the page is wise, as it's the usual thing to do when scraping. If the method is changed, re-scraping isn't necessary. The sentences? Well, on a first run, perhaps not needed. If the text is still there to scrape, it can be redone if needed to collect the sentence data, which could potentially be useful for something like text prediction. So the words then? I think that would be wise. But I must also store the Syllabic Units, seperately. I don't think I'll link them to the words, I'll just have them both there.

But how?

A previous writing, far above, proposed the following for words:

WORD(str)
WORD_COUNT(int)
NORMALIZED_WORD(str)
PRESUMED_DIALECT(enum)
RESOURCE_TYPE(enum) WEB | PDF
URL(str)

WORD and NORMALIZED_WORD, yes. The others, perhaps not. Maybe WORD_COUNT.

After some research, it looks like this won't be so bad after all. Just need to research some more DB fundamentals.

-

table.tsv
generate.py

Eventually, I hope these to be the keystones of the project. The eventual goal will be to have generate.py create, on demand, two sets of data:

1. The normalization and analysis table. This will contain all possible (lowercase) syllabic units, ie. the minimal orthographic units that are represented by syllabics. So 'ᐯ' is a unit, and 'pai' is the same unit in latin form. 'ᖅᑮ' is a unit despite being multiple characters. These will be mapped to their normalizations.

It will also contain all names and their normalizations.

It will also contain all French and English words that contain only the characters that Inuktitut words also contain. So 'and' won't be in the list, as Inuktitut lacks a 'd'. But 'mum' will be on the list. Any English words that match Inuktitut words should be excluded (probably). There's probably a few small ones. French too.

2. The conversion tables. These will be used to convert from the normalized form into syllabics (and in the case of West Nunavut, into their latin script as the h/s overlap).

generate.py should also be able to paste the tables into the code directly, when called on to do so.

===
June 19th, 2024

What might a user expect from my systems?

A web interface user is likely to want to paste some syllabics or latin text and see the inverse.

[ Convert to / from syllabics ]

Convert [to latin], [Nunavik] spelling.

---

The 'h' strikes again. I thought I had this solved, with the normalization.

ᕼ (syllabic) seems common in words like hockey (ᕼᐋᑭ, many search results)

ᕴ and its counterparts appear to be unused in practice. I would like to be able to scan for them in case they appear.

ᓭ and its counterparts are very common. In the East, they make up the "s" series, and in the West (Nattilik, for example), they make up the "h" series (words in one dialect will say 'h' in place of 's' phonetically, so the orthography remains the same).


So, I thought that in the general case, a latin 'h' would transliterate to ᕼ, perhaps even in the Nunavik dialect, though I'd research ᕴ's use of course. In regions that use ᓭ, that would be used. Perhaps exceptions could be given for words like ᕼᐋᑭ, of English origin.

The ᓭ series, when normalized to latin, could always be normalized as 's', since the dialects that use 'h' instead of 's' don't include an 's'. Therefore, their latin represntations can use the exact latin normalization with the s subbed for h. In fact I believe that would be the only latin normalization required, unless we're talking about the new latin format, which shouldn't be an issue in itself.

The new issue is with the analysis of latin text in particular. The 'h' is always 'h' in Latin text, so it can be any one of these dialects (and, as mentioned, it's possible that words like ᕼᐋᑭ bleed into dialects like Nattilik, complicating things. This is one of the ideas behind "chunking". For another example, a news article written in Nunavut might have a Nattilik name with ᖨ in it. How to determine dialect boundaries, hard to say).

In any case, I need a "this could be normalized in these three ways" data structure in the Rust code. How can I go about this? Well, no.

In fact, the whole point of the normalization is that I don't need a lot of ways to represent the same thing. Hmm. So what was the issue again?

Oh, so, when creating the analyzation map from the syllabic units, I must remember to not associate dialect with 'h'. Perhaps not with any latin letter with multiple entries, but I think h is the only exception for now. I suspect that even in dialects "without" h, words like ᕼᐋᑭ will be there.


** Interesting resource: https://kiputtijjut.pirurvik.ca/?section=about

Includes descriptions of all the transliterations, including the ᑍ series-es.

Diff of East and West transliterations:
https://www.diffchecker.com/1487oQbk/

===
June 20th, 2024

I'm stuck. My notes end up being so detailed and broad that I often start writing code to get ahead of it all. There are too many details and ideas to do everything at once. But when writing code, I can't find the thread, and I don't even understand my goals. I keep returning here and trying to figure that out.

I want to build a parser that starts at the beginning of a string and analyzes it.

Let's say it does it like this: It finds the next whitespace, perhaps with SIMD.

Check for postal code & skip (can be regex to start, sped up with SIMD later)

Check for email & skip

Check whole word against En / Fr map & skip

--> Check for Inuktitut syllabic unit

If yes, add normalized and original spelling to word buffer, goto -->

If no and not end of word, reinterpret word as non-inuktitut

If no and end of word, add word buffer to chunks

-

More simply

skip email, url, postal code


===
June 26th, 2024

Consider the following text:

Inuitmyths.com ᕿᑭᖅᑕᓂ ᐃᓄᐃᑦ ᑲᑐᔾᔨᖃᑎᒌᒃᑯᑦ ᐱᓕᕆᐊᖃᐃᓐᓇᕐᓂᐊᖅᐳᑦ ᑲᑎᖅᓱᐃᓇᓱᖕᓂᕐᒥᒃ  ᐅᓂᒃᑳᑐᖃᕐᓂᒃ ᐊᑐᐃᓐᓇᐅᑎᓪᓗᒋᓪᓗ ᐃᓄᖕᓄᑦ.  ᐅᓂᒃᑳᖃᕈᕕᑦ ᑐᓴᖅᑕᐅᖁᔭᕐᓂᒃ ᖃᐅᔨᒪᒍᕕᓪᓘᓐᓃᑦ ᐅᓂᒃᑳᓕᖕᓂᒃ, ᐅᕙᑦᑎᓐᓄᑦ ᖃᐅᔨᒃᑲᐅᑎᒋᑦ ᐅᕗᖓ info@myths.com.  ᐱᓕᕆᖃᑎᒌᒃᑯᑦᑕ, ᖁᕕᐊᓲᑎᖃᕈᓐᓇᖅᐳᒍᑦ ᓴᙱᒃᑎᒋᐊᕐᓗᑎᒍᓪᓗ ᐅᓂᒃᑳᖅᑐᐊᕈᓯᖅᐳᑦ ᐱᒻᒪᕆᐅᒻᒪᑦ ᐃᓄᐃᑦ ᐃᓕᖅᑯᓯᖓᓐᓄᑦ.

If the parser was structured like this

email
url
postal code
inuktitut
  syllabic unit, repeat

And the first four (or however many) token types were regexes, they would be run through for every word one after the other. To combine them into a tree like search structure would be to implement a full tokenizer and parser.

Perhaps it would be worth it to do a fast SIMD search beforehand.

dot_mask = [..................]
at_mask = [@@@@@@@@@@@@@@@@@@]


text = [test@example.com]

text & dot =
____________1___

text & at =
____1___________


text & space_mask
________________



** NGrams list:
https://github.com/soldierkam/langid/blob/master/src/main/resources/org/kamionowski/langid/impl/iu-3grams.txt