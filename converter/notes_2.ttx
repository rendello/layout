
I like using PHF for its efficiency, it allows me to load a hashmap at runtime without hashing and compiling all the entries. As it stands now, however, there are some pitfalls. Namely, I'm using a system of "cascading hashmaps", which feels inefficient. I would like to avoid this.

Why? The original idea for this hashmap system was TempleOS' symbol structure. In TempleOS, every single symbol in the entire OS is hashed (as I understand it), and if I saught symbol isn't found in the processes' hashmap, the parent is checked, and so on. If this works for every symbol on an OS, why can't it work for a textual system, converting tiny amounts of text, with the only speed requirement being "the user doesn't notice"?

There's really no reason to do it like this. Surely, CSS uses a similar datastructure for its "cascading" rules?

Besides, in reality, I'm going to be allocating and copying a fair bit. Like, I have to lowercase the text I'm processing, which is a whole other issue in itself.

I believe my frustration is with the "impurity" of it all.

If I were to use a regular hashmap, I would update it dynamically based on the ruleset I wanted. For example, replacing the H syllabic, or loading/unloading the "ai" series.

I have an idea that would combine that last idea with the efficiency of PHF, though I'm not sure if it's possible, or even a good idea.

I would create perfect hashes for all the entries at once, ensuring no collisions, and then I'd selectively create rulesets from that big map. For example, I would have the "ai" series hashed, but not include that in the final loaded hashmap if the dialect didn't use the series.

Of course, the duplicate keys thing is an issue. The H series could have multiple results based on dialect. One idea could be to keep the compiled hash, but swap out the returned value. Not sure if that's possible or even practical.

What's Rust actually doing when loading a PHF hashmap? What would my "dynamic but not" hashmaps even look like, how would compiling them work?

It's clear that creating this system would add a lot of complexity with no little upside, so how is that "more pure"? I believe the reason I'm obsessing over this is that I'm anxious about what to do next in the project, particularly how to deal with dialects in the code, and how advanced I want the WASM interface to be.

I think I need to look at the exceptions listed on the Wikipedia chart, and decide what I need to do with each.

---

[o] Nunavik -- "ai" column -- Option
[o] Nunavik -- ·ïπ/h series -- H Option
[ ] Nunavut -- ·ñÖ·ë≤ -> qqa (qq series) -- Unambiguous (I think)
[ ] Nunavik -- ·ñì, ·ôµ look different -- Font issue, ignore.

[o] Nunavut -- ·ïº -> H -- H Option
[o] Nunavut (East) -- ·ì¥ -> s -- *
[o] Nunavut (West) -- ·ì¥ -> h -- *

[ ] Nattilik -- ë™∫ -> ≈°a -- Unambiguous
[o] Nattilik -- ë™¥ -> ha -- H Option **
[ ] Nattilik -- ·ñ¨ -> ≈ôa -- Unambiguous
[o] Nattilik -- ·ñì, ·ôµ -> ≈ã, ≈ã≈ã -- Option

[ ] Many dialects -- ·ñ§ -> ≈Ça -- Unambiguous
[ ] Aivilik -- ·ñØ -> b -- Unambiguous (probably safe enough)

* I don't know what to make of ·ì¥, I think I'll have to do some research.

** Is this used differently than the standard Nunavut and Nunavik "H" options?
I want to look into the use of H syllabics, how commona are they.

Added with the Kevin King proposal, will look into it.

For ≈° series, I think shr should be an unambiguous normalization, since it seems common, but less preferred to ≈°.

---

left hand corner could be doubled consonant

qaa qi  qii
qa  q   qu
qq  qai quu
‚Üë‚Üë

·ñÑ ·ïø ·ñÄ
·ñÉ ·ñÖ  ·ñÅ
·ñÖ·íÉ ·ôØ ·ñÇ
‚Üë‚Üë

Might make more sense, since doubled consonants are often different
than just putting the consonant twice, ie. ·ñÖ

---

Add & as ≈Ç normalization(?) Maybe overzealous.


https://www.itk.ca/projects/inuktut-qaliujaaqpait-converter/
Converts "·ñ¨, ë™¥, ë™∫, ·ñ§" to "rha, ha, shra, hla". Should there be an option for this? Should this just be a normalization? I think this is the new "standard" for latin letters, but I'm not sure if it's really preferred.

This is the "Qaliujaaqpait", the unified system. I think it should be an option, but perhaps not the primary one.

---

As for the normalization (lowercasing) of latin text before conversion:

A naive approach would be to lowercase all the text first, then run the conversions. I don't love this as it requires reallocation, but also, any unconverted characters lose their case, which is not ideal. In reality, I suppose I don't hope a whole lot of latin characters will go through, as I hope to automatically detect Inuktitut words and only convert those, in most cases.

But still, it should keep the case.

What I think I should do is re-engineer the hashmap stuff, so instead of operating on bytes, it operates on characters. If my max byte count is 5, for example, I can check if the characters fit inside those bounds (with my next_jump function), and if they don't, then skip. Actually, I can look at the bounds of the characters, then union those two arrays, ie. If my KEY_LENGTHS are [6, 5, 4, 2], and the bounds of the chars are only [4, 2], then I can skip 5 and 6. Hell, if the next char is 1, I can skip all checks!

If the bounds are ok, I should probably just load those bytes into a register / temp var, then go through and lowercase the chars in place. Caveat: lowercasing in place is not usually done, since there can be different byte lengths. However, for the chars I'm looking for, I don't *think* that's the case. Therefore, I should write my own little lowecasing function, and use tests / prop tests to make SURE the lengths won't change.

That way I can skip a lot more checks, and I don't have to reallocate to do the lowercasing. On the other hand, I'll be doing a lot more lowercasing, and it will probably not be great for cache locality. I don't think this will matter though, the speed will not be noticable, and the main thing is that case is preserved for non-inuk letters.

Speaking of, of course, I will not be using my register loaded values if the non-conversion path is taken, just standard skip and add to str buff.

I mean, I could probably use some soft of intelligent copying to make sure I don't relower the same letters over and over. I guess I should, it will probably just be a little bit of logic.


Should I just use a u64 to put the bytes in? For now, my biggest KEY_LENGTH is 6. I don't think there will be much more added, maybe some normalizations. I can see the byte number reaching 8, possibly with the "Qaliujaaqpait" normalizations. Well no, they're all ascii, and none are 8 long. I think we're safe for now, anyway. I guess in that case, I could make the hashmap work un u64s, instead of u8s. It would make maps.rs less pretty, but more efficient since we'd no longer be pointer chasing. Again, not that it matters.

Ok, ‚±¢ and …´ have different byte lengths. I don't think this should matter, just as long as I can fit the normalized stuff in the buffer. I'm not married to a u64, I just don't want to allocate a string. I'll have to keep the original and the buffer's indices separately.

I mean Rust has a u128, I could use that if the u64 doesn't work out. I should just prop test it.